<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ma Page Web</title>
    <link rel="stylesheet" href="Dark PDF export.css">
	<link rel="stylesheet" href="supercharged-links-gen.css">
</head>
<body>
<h1>DÃ©ploiement AutomatisÃ© d'un Site de Documentation avec MCO/MCS</h1>
<p>Valentin LEGRAND, devoir rendu le 20 juin 2025</p>
<hr class="gradient-warm">
<h2>Phase 1 : PrÃ©paration des Machines</h2>
<hr class="gradient-warm">
<h3>ğŸ”¹ Ã‰tape 1 : Installation de l'OS</h3>
<p>âœ… <strong>Choix de l'OS (Ubuntu/Debian recommandÃ©)</strong> :</p>
<pre><code class="language-conf">Debian 12` (recommandÃ©) 
</code></pre>
<p>ğŸ› ï¸ğŸ” <strong>Installation de base avec SSH activÃ©</strong> :</p>
<pre><code class="language-conf">Installation par dÃ©faut avec la suppression de la partition swap pour kubernetes 
</code></pre>
<p><strong>SSH activÃ©</strong> :</p>
<pre><code class="language-bash">ssh.service - OpenBSD Secure Shell server
Loaded: loaded (/lib/systemd/system/ssh.service; enabled)
Active: active (running) since Tue 2025-04-15 09:29:13 CEST; 5min ago
</code></pre>
<p>ğŸ“¦ <strong>Mise Ã  jour des paquets</strong> :</p>
<pre><code class="language-bash">apt-get update
apt-get upgrade
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tape 2 : Configuration initiale des machines</h3>
<p>ğŸ‘¥ <strong>Configuration du PATH (commandes manquantes)</strong> :</p>
<pre><code class="language-bash">export PATH=$PATH:/usr/sbin #Ajout de ce dossier au path pour avoir des commandes nÃ©cessaire Ã  la suite du projet
</code></pre>
<p>ğŸ‘¥ <strong>Ajout des utilisateurs</strong> :</p>
<pre><code class="language-bash">adduser val_master       # VM Master
adduser val_worker1      # VM Worker Node 1
adduser val_worker2      # VM Worker Node 2
</code></pre>
<p>ğŸ‘¥ <strong>Ajout des utilisateurs au groupe sudo</strong> :</p>
<pre><code class="language-bash">usermod -aG sudo val_master      # VM Master
usermod -aG sudo val_worker1     # VM Worker Node 1
usermod -aG sudo val_worker2      # VM Worker Node 2
</code></pre>
<p>ğŸŒ <strong>Configuration rÃ©seau &amp; hostnames</strong> :</p>
<pre><code class="language-bash">hostnamectl set-hostname Master     #VM Master
hostnamectl set-hostname Worker1    #VM Worker Node 1
hostnamectl set-hostname Worker2    #VM Worker Node 2
</code></pre>
<p>ğŸ” <strong>GÃ©nÃ©ration de clÃ© SSH + copie</strong> :</p>
<pre><code class="language-bash">ssh-keygen -t rsa -b 4096   # Avec passphrase &quot;valentin&quot;

ssh-copy-id val_worker1@192.168.142.144
ssh-copy-id val_worker2@192.168.142.143
ssh-copy-id val_master@192.168.142.137
</code></pre>
<p>ğŸ§¾ <strong>RÃ©solution de nom</strong> (<code>/etc/hosts</code>) :</p>
<pre><code class="language-bash">192.168.142.137 Master
192.168.142.144 Worker1
192.168.142.143 Worker2
</code></pre>
<p>ğŸ§¾ **Fichier de configuration SSH ** (<code>/etc/ssh/sshd_config</code>) :</p>
<pre><code class="language-bash">PubKeyAuthentication yes
PermitRootLogin no
PasswordAuthentication no
</code></pre>
<p>ğŸ§ª <strong>VÃ©rification SSH sans mot de passe via des clÃ©s publiques</strong> :</p>
<pre><code class="language-bash">ssh val_worker1@192.168.142.142
# âœ Enter passphrase...

ssh root@192.168.142.142
# âœ Permission denied (clÃ© requise)
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tape 3 : Installation des prÃ©requis</h3>
<p>ğŸ **Installation de python ** :</p>
<pre><code class="language-bash">sudo apt install python3 python3-pip
</code></pre>
<p>âœ… <strong>VÃ©rification de la connectivitÃ© entre les machines</strong> :</p>
<pre><code class="language-bash">val_master# ping 192.168.142.144
val_master# ping 192.168.142.143
</code></pre>
<hr>
<hr class="gradient-warm">
<h2>Phase 2 : Installation &amp; Configuration de Docker</h2>
<p><hr class="gradient-warm">    <!-- Ligne jaune/orange --></p>
<h3>ğŸ”¹ Ã‰tape 1 : Installation de Docker</h3>
<p>ğŸ“š <a href="https://www.it-connect.fr/installation-pas-a-pas-de-docker-sur-debian-11/">Guide IT-Connect Docker Debian</a></p>
<pre><code class="language-bash">sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
sudo curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
sudo echo &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io
sudo systemctl enable docker
sudo systemctl status docker
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tape 2 : Ajout des utilisateurs Ã  Docker (pour Ã©viter lâ€™utilisation de sudo)</h3>
<p>ğŸ‘¤ <strong>Ajouter l'utilisateur au groupe <code>docker</code></strong> :</p>
<pre><code class="language-bash">usermod -aG docker ${USER}
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tape 3 : VÃ©rification du bon fonctionnement de Docker</h3>
<p>âœ… <strong>Tester le fonctionnement</strong> :</p>
<pre><code class="language-bash">docker run hello-world
</code></pre>
<hr>
<hr class="gradient-warm">
<h2>Phase 3 : DÃ©ploiement de Kubernetes</h2>
<hr class="gradient-warm">
<h3>ğŸ”¹ Ã‰tape 1 : Installation de Kubernetes (kubeadm, kubelet, kubectl)</h3>
<p>ğŸ› ï¸ <strong>PrÃ©paration</strong> :</p>
<pre><code class="language-bash">sudo modprobe br_netfilter
</code></pre>
<p>ğŸ“š <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Installer Kubernetes via kubeadm</a></p>
<pre><code class="language-bash">sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
sudo systemctl enable --now kubelet
</code></pre>
<p>ğŸ“¦ <a href="https://github.com/Mirantis/cri-dockerd/releases/tag/v0.3.17">TÃ©lÃ©charger cri-dockerd</a></p>
<pre><code class="language-bash">wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.17/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
sudo dpkg -i cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tape 2 : Initialisation du cluster</h3>
<p>ğŸš€ <strong>Initialiser le cluster</strong> :</p>
<pre><code class="language-bash">sudo kubeadm init --cri-socket unix:///var/run/cri-dockerd.sock
</code></pre>
<p>ğŸ”§ <strong>Configurer kubectl</strong> :</p>
<pre><code class="language-bash"># Si on est en root
export KUBECONFIG=/etc/kubernetes/admin.conf

# Si on est pas root
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tape 3 : Ajout des nÅ“uds au cluster</h3>
<p><strong>Connexion des workers au noeud master</strong> :</p>
<pre><code class="language-bash">kubeadm join 192.168.142.137:6443 --token lyu32e.828538t417gn9omw \
	--discovery-token-ca-cert-hash sha256:d49e4ea40e6f390e95ad78ce10441809a5a8fb56eccbbac2f33f89fb2ea81137 --cri-socket unix:///var/run/cri-dockerd.sock
</code></pre>
<p>ğŸŒ <strong>DÃ©ployer Flannel (rÃ©seau Pods)</strong> :</p>
<pre><code class="language-bash">kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml # Sur le master
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tape 4 : VÃ©rification du bon fonctionnement du cluster</h3>
<p>ğŸ“‹ <strong>VÃ©rification</strong> :</p>
<pre><code class="language-bash">kubectl get nodes
kubectl get pods
</code></pre>
<hr>
<hr class="gradient-warm">
<h2>Phase 4 : DÃ©ploiement du Site Web</h2>
<hr class="gradient-warm">
<h3>ğŸ”¹ Ã‰tapes 1 : Ã‰criture dâ€™un Dockerfile (pour contenir lâ€™application web statique)</h3>
<p>ğŸ“ <strong>Ã‰criture du <code>Dockerfile</code> pour app web statique</strong> :</p>
<pre><code class="language-bash"># Utiliser la derniÃ¨re image officielle de Nginx
FROM nginx:latest

# Copier le rÃ©pertoire contenant le html,css,js dans le rÃ©pertoire de service de Nginx
COPY Website_content /usr/share/nginx/html 

# Copier notre default.conf dans le rÃ©pertoire de service de Nginx
COPY Config/nginx/conf/default.conf /etc/nginx/conf.d/default.conf

# Copier le fichier PDF dans le rÃ©pertoire de service de Nginx
COPY Website_content/rapport.pdf /usr/share/nginx/html/mon_fichier.pdf

# Copier les certificats SSL dans le rÃ©pertoire de service de Nginx
COPY Config/nginx/certs/server.crt /etc/nginx/certs/server.crt
COPY Config/nginx/certs/server.key /etc/nginx/certs/server.key
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tapes 2 : CrÃ©ation dâ€™un docker-compose.yml pour lâ€™orchestration locale</h3>
<p>ğŸ“œ <strong>CrÃ©ation du <code>docker-compose.yml</code></strong> :</p>
<pre><code class="language-bash">services:
  static-app:
    build: .
    ports:
      - &quot;80:80&quot;
      - &quot;443:443&quot;
    restart: always
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tapes 3 : DÃ©ploiement initial avec Docker Compose pour validation</h3>
<p>ğŸ§ª <strong>DÃ©ploiement local avec Docker Compose</strong> :</p>
<pre><code class="language-bash">docker compose up --build
</code></pre>
<p>âœ…** Test des accÃ¨s au site :</p>
<p>Sur les images, le 2 est aprÃ¨s test, il s'agit de l'ancienne url.</p>
<p>https://mon-site-local.test:443
http://mon-site-local.test:80</p>
<img src="Pasted image 20250604214223.png" />
<img src="Pasted image 20250604214528.png" />
<hr>
<h3>ğŸ”¹ Ã‰tapes 4 : CrÃ©ation des manifests Kubernetes pour le dÃ©ploiement sur le cluster</h3>
<p>ğŸš¨ğŸš¨ğŸš¨  <strong>SchÃ©ma de l'architecture logiciel :</strong></p>
<img src="Pasted image 20250620000359.png" />
<p>ğŸ³ **Push de l'image docker issue du <code>Dockerfile</code> de l'Ã©tape 1 sur docker hub afin de pouvoir l'utiliser directement dans les pods.</p>
<p>ğŸ“œ <strong>CrÃ©ation du  deployment<code>site-deployment.yaml</code></strong> :</p>
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  name: site-doc-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: site-doc
  template:
    metadata:
      labels:
        app: site-doc
    spec:
      containers:
      - name: site
        image: valouze14/valouze14:latest
        ports:
        - containerPort: 80
        - containerPort: 443
</code></pre>
<p>ğŸ“œ <strong>CrÃ©ation du service <code>site-service.yaml</code></strong> :</p>
<pre><code class="language-bash">apiVersion: v1
kind: Service
metadata:
  name: site-doc-service 
spec:
  selector:
    app: site-doc # Cible tous les pods qui ont le labels site-doc
  type: NodePort
  ports:
    - name: http
      protocol: TCP
      port: 8080 # Le port sur lequel le service Ã©coutera Ã  l'intÃ©rieur du cluster
      targetPort: 80 # Le port sur lequel votre application Ã©coute Ã  l'intÃ©rieur du pod (port 80 de votre conteneur)
      nodePort: 30080 # Un port entre 30000 et 32767. Choisissez-en un disponible.
    - name: https
      protocol: TCP
      port: 4443
      targetPort: 443
      nodePort: 30443
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tapes 5 : DÃ©ploiement final sur Kubernetes</h3>
<p>ğŸš€ <strong>DÃ©ploiement final sur Kubernetes</strong> :</p>
<pre><code class="language-bash">kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/Site
</code></pre>
<p>âœ…<strong>Test des accÃ¨s au site :</strong></p>
<p>https://mon-site-local.test2:30443
http://mon-site-local.test2:30080</p>
<img src="Pasted image 20250618151743.png" />
<img src="Pasted image 20250618152106.png" />
<p>âœ…VÃ©rification load balancer :**</p>
<p>Par dÃ©faut, le service distribue sans affinitÃ© aux pods associÃ©s ce qui fait un load-balancer naturel, je peux vÃ©rifier en regardant en direct les logs des trois pods qui hÃ©berge mon site. A chaque fois, il faut supprimer les donnÃ©es dans le cache pour Ã©tablir une nouvelle session. Je constate qu'Ã  chaque nouvelle session, les requÃªtes n'arrivent jamais sur le mÃªme pod.</p>
<pre><code class="language-bash">kubectl logs -f site-doc-deployment-7887cccc5b-mp5vt
</code></pre>
<p><img src="Pasted image 20250619112740.png" /></p>
<hr>
<hr class="gradient-warm">
<h2>Phase 5 : Automatisation avec Ansible</h2>
<p><hr class="gradient-warm">    <!-- Ligne jaune/orange --></p>
<p>ğŸ“¦ <strong>DÃ©pÃ´t des ressources sur Github</strong></p>
<pre><code class="language-bash">git clone https://github.com/Valouze14/Projet.git
</code></pre>
<p>ğŸ“œ<strong>CrÃ©ation du fichier d'inventories.ini</strong> :</p>
<pre><code class="language-bash">[all]
192.168.142.137 ansible_user=val_master # VM Master
192.168.142.144 ansible_user=val_worker1 # VM Worker Node 1
192.168.142.143 ansible_user=val_worker2 # VM Worker Node 2

[masters]
192.168.142.137

[workers]
192.168.142.144
192.168.142.143
</code></pre>
<h3>ğŸ”¹ Ã‰tapes 1 : CrÃ©ation des playbooks Ansible pour automatiser lâ€™installation et la configuration</h3>
<p>ğŸ“œ <strong>CrÃ©ation du fichier setup-hosts.yml</strong> :</p>
<pre><code class="language-bash">
- name: Setup Hosts
  hosts: all
  become: true
  tasks:

############################################################
# Ajouter les entrÃ©es des hÃ´tes et le nom de domaine dans le fichier /etc/hosts
############################################################
  - name: DÃ©finir les entrÃ©es des hÃ´tes + nom de domaine du site web
    blockinfile:
      path: /etc/hosts
      block: |
        192.168.142.137 Master
        192.168.142.144 Worker1
        192.168.142.143 Worker2
        192.168.142.137 mon-site-local.test
      backup: true

############################################################
# DÃ©finir le nom dâ€™hÃ´te pour la VM Master
############################################################
  - name: DÃ©finir le nom d hÃ´te pour la VM Master
    hostname:
      name: Master
    when: inventory_hostname == '192.168.142.137'

############################################################
# DÃ©finir le nom dâ€™hÃ´te pour la VM Worker1
############################################################
  - name: DÃ©finir le nom d hÃ´te pour la VM Worker 1
    hostname:
      name: Worker1
    when: inventory_hostname == '192.168.142.144'

############################################################
# DÃ©finir le nom dâ€™hÃ´te pour la VM Worker2
############################################################
  - name: DÃ©finir le nom d hÃ´te pour la VM Worker 2
    hostname:
      name: Worker2
    when: inventory_hostname == '192.168.142.143'

############################################################
# Installer Python 3 et pip (prÃ©requis pour d autres tÃ¢ches Ansible)
############################################################
  - name: Installer python3 et python3-pip
    apt:
      name:
        - python3
        - python3-pip

############################################################
# Charger le module noyau br_netfilter (nÃ©cessaire pour Kubernetes)
############################################################
  - name: Charger le module br_netfilter
    modprobe:
      name: br_netfilter
      state: present

############################################################
# Mettre Ã  jour la liste des paquets et effectuer une mise Ã  jour complÃ¨te
############################################################
  - name: Mise Ã  jour de la liste des paquets
    apt:
      update_cache: true
      upgrade: dist
</code></pre>
<p>ğŸ“œ <strong>CrÃ©ation du fichier install-docker.yml</strong> :</p>
<pre><code class="language-bash">- name: Install Docker 
  hosts: all
  become: true
  tasks:

############################################################
# Installer les paquets nÃ©cessaires au fonctionnement de Docker
############################################################
  - name: Installer les paquets nÃ©cessaires pour Docker
    apt:
      name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg2
      - software-properties-common
      state: present
      update_cache: true

############################################################
# Ajouter la clÃ© GPG officielle de Docker pour sÃ©curiser le dÃ©pÃ´t
############################################################
  - name: Ajouter la clÃ© GPG officielle de Docker
    shell: curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    args:
      creates: /usr/share/keyrings/docker-archive-keyring.gpg

############################################################
# Ajouter le dÃ©pÃ´t officiel Docker Ã  la liste des sources APT
############################################################
  - name: Ajouter le dÃ©pÃ´t Docker Ã  la liste des sources APT
    lineinfile:
      path: /etc/apt/sources.list.d/docker.list
      line: deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian {{ ansible_distribution_release }} stable
      create: true
      mode: '0644'

############################################################
# Mettre Ã  jour lâ€™index des paquets aprÃ¨s ajout du dÃ©pÃ´t Docker
############################################################
  - name: Mettre Ã  jour l'index des paquets aprÃ¨s l'ajout du dÃ©pÃ´t Docker
    apt:
      update_cache: true

############################################################
# Installer Docker Engine, l'interface en ligne de commande et containerd
############################################################
  - name: Installer Docker Engine, CLI et Containerd
    apt:
      name:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      state: present

############################################################
# Activer et dÃ©marrer le service Docker au dÃ©marrage de la machine
############################################################
  - name: VÃ©rifier que le service Docker est dÃ©marrÃ© et activÃ© au dÃ©marrage
    systemd:
      name: docker
      state: started
      enabled: true

############################################################
# VÃ©rifier le statut du service Docker (diagnostic uniquement)
############################################################
  - name: VÃ©rifier le statut du service Docker (pour information, ne modifie rien)
    command: systemctl status docker
    register: docker_status

############################################################
# Afficher le statut du service Docker dans la sortie Ansible
############################################################
  - name: Afficher le statut du service Docker
    debug:
      var: docker_status.stdout_lines

############################################################
# Ajouter le user val_master au groupe docker (accÃ¨s sans sudo)
############################################################
  - name: Ajouter val_master au groupe docker
    user:
      name: val_master
      groups: docker
      append: true
    when: inventory_hostname == '192.168.142.137'

############################################################
# Ajouter le user utilisateur val_worker1 au groupe docker
############################################################
  - name: Ajouter val_worker1 au groupe docker
    user:
      name: val_worker1
      groups: docker
      append: true
    when: inventory_hostname == '192.168.142.144'

############################################################
# Ajouter l'utilisateur val_worker2 au groupe docker
############################################################
  - name: Ajouter val_worker2 au groupe docker
    user:
      name: val_worker2
      groups: docker
      append: true
    when: inventory_hostname == '192.168.142.143'

############################################################
# VÃ©rifier que Docker fonctionne avec un conteneur de test
############################################################
  - name: ExÃ©cuter docker run hello-world pour vÃ©rifier la bonne installation
    command: docker run hello-world
    become: false
    register: docker_hello_world_result

############################################################
# Afficher le rÃ©sultat du test docker run hello-world
############################################################
  - name: Afficher le rÃ©sultat de docker run hello-world
    debug:
      var: docker_hello_world_result.stdout_lines

############################################################
# TÃ©lÃ©charger le binaire cri-dockerd depuis GitHub
############################################################
  - name: TÃ©lÃ©charger le paquet cri-dockerd
    get_url:
      url: https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.17/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
      dest: /tmp/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
      mode: '0644'

############################################################
# Installer le paquet cri-dockerd tÃ©lÃ©chargÃ© localement
############################################################
  - name: Installer le paquet cri-dockerd
    ansible.builtin.apt:
      deb: /tmp/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb

############################################################
# Lancer Docker Compose pour dÃ©ployer les conteneurs du projet
############################################################
  - name: Lancer docker compose up avec le rÃ©pertoire de projet spÃ©cifiÃ©
    ansible.builtin.command: docker compose up --build -d
    args:
      chdir: /home/val_master/Documents/Projet/Project-docs/   # SpÃ©cifie le rÃ©pertoire de travail
    when: inventory_hostname == '192.168.142.137' # ExÃ©cuter uniquement sur le nÅ“ud maÃ®tre
</code></pre>
<p>ğŸ“œÂ <strong>CrÃ©ation du fichier install-k8s.yml</strong>Â :</p>
<pre><code class="language-bash">############################################################
# 1. Installation des prÃ©requis Kubernetes
############################################################
- name: Installer les paquets et composants Kubernetes
  hosts: all
  become: true
  tasks:
    - name: Installer les paquets nÃ©cessaires
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gpg
        state: present

    - name: Ajouter la clÃ© GPG officielle de Kubernetes
      shell: &gt;
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key |
        gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: Ajouter le dÃ©pÃ´t Kubernetes
      lineinfile:
        path: /etc/apt/sources.list.d/kubernetes.list
        line: deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
        create: true
        mode: '0644'

    - name: Mise Ã  jour des paquets
      apt:
        update_cache: true

    - name: Installer kubelet, kubeadm et kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present

    - name: Marquer kubelet, kubeadm et kubectl en hold
      dpkg_selections:
        name: '{{ item }}'
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: Activer et dÃ©marrer le service kubelet
      systemd:
        name: kubelet
        enabled: true
        state: started


############################################################
# 2. Initialisation du master Kubernetes
############################################################
- name: Initialiser le master Kubernetes
  hosts: 192.168.142.137
  become: true
  tasks:
    - name: Initialiser le cluster avec kubeadm
      command: kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///var/run/cri-dockerd.sock
      tags: [kubeadm_init]

    - name: CrÃ©er .kube/config pour le user val_master
      shell: |
        mkdir -p /home/val_master/.kube
        cp -i /etc/kubernetes/admin.conf /home/val_master/.kube/config
        chown val_master:val_master /home/val_master/.kube/config

    - name: Appliquer le manifeste Flannel CNI
      ansible.builtin.shell:
        kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
      become: false

############################################################
# 3. RÃ©cupÃ©ration de la commande de jointure
############################################################
- name: Obtenir la commande de jointure
  hosts: 192.168.142.137
  become: true
  tasks:
    - name: GÃ©nÃ©rer la commande kubeadm join
      command: kubeadm token create --print-join-command
      register: join_command
      run_once: true

    - name: Sauvegarder la commande dans un fact partagÃ©
      set_fact:
        kube_join_cmd: &quot;{{ join_command.stdout }} --cri-socket unix:///var/run/cri-dockerd.sock&quot;
      run_once: true


############################################################
# 4. Faire joindre les workers au cluster
############################################################
- name: Faire rejoindre les workers au cluster Kubernetes
  hosts: all
  become: true
  tasks:
    - name: Joindre les workers (sauf master)
      command: &quot;{{ hostvars['192.168.142.137'].kube_join_cmd }}&quot;
      when: inventory_hostname != '192.168.142.137'
      register: kubeadm_join_output_workers
      tags: [kubeadm_join_workers]


############################################################
# (Facultatif) Copier admin.conf depuis le master si nÃ©cessaire
############################################################
- name: Copier admin.conf sur les workers depuis localhost (optionnel)
  hosts: workers
  become: true
  tasks:
    - name: Rendre admin.conf lisible en local
      file:
        path: /etc/kubernetes/admin.conf
        mode: '0644'
        owner: root
        group: root
      delegate_to: localhost
      run_once: true

    - name: Copier admin.conf sur les workers
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /etc/kubernetes/admin.conf
        owner: root
        group: root
        mode: '0644'

############################################################
# 5. Configuration de .kube/config sur les workers (facultatif)
############################################################
- name: Configurer kubectl sur les workers (facultatif)
  hosts: workers
  become: true
  tasks:
    - name: CrÃ©er le rÃ©pertoire .kube et copier le fichier admin.conf (Master uniquement)
      ansible.builtin.shell: |
        mkdir -p /home/val_worker1/.kube
        sudo cp -i /etc/kubernetes/admin.conf /home/val_worker1/.kube/config
        sudo chown val_worker1:val_worker1 /home/val_worker1/.kube/config
      when: inventory_hostname == '192.168.142.144'

    - name: CrÃ©er le rÃ©pertoire .kube et copier le fichier admin.conf (Master uniquement)
      ansible.builtin.shell: |
        mkdir -p /home/val_worker2/.kube
        sudo cp -i /etc/kubernetes/admin.conf /home/val_worker2/.kube/config
        sudo chown val_worker2:val_worker2 /home/val_worker2/.kube/config
      when: inventory_hostname == '192.168.142.143'
</code></pre>
<p>ğŸ“œÂ <strong>CrÃ©ation du fichier install-k8s-manifests.yml</strong>Â (inclus la phase 6 pour la supervision):</p>
<pre><code class="language-bash">---
- name: DÃ©ploiement des outils de monitoring et des applications Kubernetes (sans collections externes)
  hosts: masters

  tasks:

############################################################
# CrÃ©er une ConfigMap Prometheus Ã  partir dâ€™un fichier de configuration local
############################################################
    - name: CrÃ©er la ConfigMap Prometheus
      ansible.builtin.command:
        kubectl create configmap prometheus-config --from-file=/home/val_master/Documents/Projet/Project-docs/Kubernetes/Config-map/prometheus.yml 

############################################################
# DÃ©ployer les manifests Kubernetes de l'application web (site)
############################################################
    - name: Appliquer les manifestes Kubernetes pour site
      ansible.builtin.command: kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/site

############################################################
# DÃ©ployer les manifests Kubernetes de Prometheus (monitoring)
############################################################
    - name: Appliquer les manifestes Kubernetes pour prometheus
      ansible.builtin.command: kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/prometheus

############################################################
# DÃ©ployer les manifests Kubernetes de Grafana (dashboard)
############################################################
    - name: Appliquer les manifestes Kubernetes pour grafana
      ansible.builtin.command: kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/grafana

############################################################
# DÃ©ployer les manifests Kubernetes de kube-metrics (exporter de mÃ©triques)
############################################################
    - name: Appliquer les manifestes Kubernetes pour kube-metrics
      ansible.builtin.command: kubectl apply -k /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/kube-metrics


- name: Configurer kubectl sur les workers (facultatif)
  hosts: workers
  become: true

  tasks:

############################################################
# DÃ©marrer le conteneur cAdvisor pour surveiller les performances du nÅ“ud
############################################################
    - name: DÃ©marrer le conteneur cAdvisor
      ansible.builtin.shell: |
        docker run -d \
          --name=cadvisor \
          --restart=unless-stopped \
          --volume=/:/rootfs:ro \
          --volume=/var/run:/var/run:ro \
          --volume=/sys:/sys:ro \
          --volume=/var/lib/docker/:/var/lib/docker:ro \
          -p 8080:8080 \
          gcr.io/cadvisor/cadvisor
        # Ajout d'une vÃ©rification pour l'idempotence (pas strictement nÃ©cessaire pour 'docker run -d' car il ne relancera pas si le conteneur existe dÃ©jÃ )
      args:
        creates: /var/run/docker/cadvisor.pid

</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tapes 2 : ExÃ©cution des playbooks pour provisionner lâ€™ensemble des machines</h3>
<p>â–¶ï¸ <strong>ExÃ©cution des playbooks sur le parc</strong> :</p>
<pre><code class="language-bash">ansible-playbook Projet/Project-docs/Playbooks/All.yml -i Projet/Ansible/hosts.ini -K
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tapes 3 : Validation du bon fonctionnement</h3>
<p>âœ… <strong>Validation de lâ€™installation automatisÃ©e</strong> :</p>
<img src="Pasted image 20250618155144.png" />
<hr class="gradient-warm">
<h2>Phase 6 : Maintenance et Supervision</h2>
<hr class="gradient-warm">
<h3>ğŸ”¹ Ã‰tapes 1 : Mise en place de la supervision des conteneurs et du cluster Kubernetes</h3>
<h5>DÃ©ploiement de mÃ©triques des conteneurs avec <code>prometheus</code>** :</h5>
<p>ğŸ“œÂ <strong>CrÃ©ation du config-map prometheus.yml</strong></p>
<pre><code class="language-bash">global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'cadvisor-worker1'
    static_configs:
      - targets: ['192.168.142.144:8080']

  - job_name: 'cadvisor-worker2'
    static_configs:
      - targets: ['192.168.142.143:8080']
  - job_name: k8s-kube-state-metrics-cluster
    honor_timestamps: true
    metrics_path: /metrics
    scheme: http
    static_configs:
      - targets: ['192.168.142.137:30000']
    metric_relabel_configs:
    - target_label: cluster
      replacement: YourDefinedK8scluster
</code></pre>
<p>ğŸ“œÂ <strong>CrÃ©ation du deployment prometheus-deployment.yaml</strong></p>
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus
        args:
          - &quot;--config.file=/etc/prometheus/prometheus.yml&quot;
        ports:
          - containerPort: 9090
        volumeMounts:
          - name: config-volume
            mountPath: /etc/prometheus/
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-config
</code></pre>
<p>ğŸ“œÂ <strong>CrÃ©ation du service prometheus-service.yaml</strong></p>
<pre><code class="language-bash">apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
spec:
  selector:
    app: prometheus
  ports:
    - protocol: TCP
      port: 9090
      targetPort: 9090
      nodePort: 30200 
  type: NodePort
</code></pre>
<h5>Mise en place de la supervision avec <code>grafana</code>** :</h5>
<p>ğŸ“œÂ <strong>CrÃ©ation du deployment grafana-deployment.yaml</strong></p>
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana
          ports:
            - containerPort: 3000
</code></pre>
<p>ğŸ“œÂ <strong>CrÃ©ation du service grafana-service.yaml</strong></p>
<pre><code class="language-bash">apiVersion: v1
kind: Service
metadata:
  name: grafana-service
spec:
  selector:
    app: grafana
  ports:
    - protocol: TCP
      port: 3000
      targetPort: 3000
      nodePort: 30500
  type: NodePort
</code></pre>
<hr>
<h3>ğŸ”¹ Ã‰tapes 2 : DÃ©ploiement de mÃ©triques et logs pour surveiller lâ€™infrastructure</h3>
<p>ğŸ³ <strong>DÃ©ploiement de mÃ©triques des conteneurs avec <code>cAdvisor</code></strong> :</p>
<pre><code class="language-bash">docker run -d \
  --name=cadvisor \
  --restart=unless-stopped \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  -p 8080:8080 \
  gcr.io/cadvisor/cadvisor
</code></pre>
<p>ğŸ“ˆ <strong>DÃ©ploiement de mÃ©triques du cluster Kubernetes avec le service <code>kube-state-metric</code></strong> :</p>
<p>ğŸ“¦ https://github.com/kubernetes/kube-state-metrics/tree/main/examples/standard</p>
<p>âœ… **VÃ©rification de la configuration de <code>prometheus</code> :
<img src="Pasted image 20250619110512.png" />
âœ… **VÃ©rification de la supervision avec <code>grafana</code> :</p>
<p>ID template utilisÃ© pour la supervision du cluster Kubernetes : 13332</p>
<img src="Pasted image 20250619110538.png" />
<p>ID template utilisÃ© pour la supervision les conteneurs : 14282</p>
<img src="Pasted image 20250619110645.png" />
<hr>
<h3>ğŸ”¹ Ã‰tapes 3 : Automatisation de la gestion des mises Ã  jour avec Ansible</h3>
<p>ğŸ” <strong>Automatisation des MAJ avec Ansible</strong> :</p>
<p>ğŸ“œÂ <strong>CrÃ©ation du playbook mco.yaml</strong></p>
<pre><code class="language-bash">---
- name: MCO
  hosts: all
  become: true

  tasks:
    - name: Mettre Ã  jour les paquets et nettoyer le cache (Debian/Ubuntu)
      ansible.builtin.apt:
        upgrade: dist
        update_cache: yes
        autoclean: yes
        autoremove: yes
      register: apt_update_result

    - name: RedÃ©marrer si le noyau a Ã©tÃ© mis Ã  jour et si nÃ©cessaire (Debian/Ubuntu)
      ansible.builtin.reboot:
        reboot_timeout: 600
      when:
        - apt_update_result.reboot_required is defined and apt_update_result.reboot_required

    - name: Nettoyer les fichiers temporaires dans /tmp (tous les OS)
      ansible.builtin.command: find /tmp -type f -atime +1 -delete
      changed_when: false
</code></pre>
<hr class="gradient-warm">
<h2>Axes d'amÃ©liorations</h2>
<hr class="gradient-warm">
<pre><code class="language-bash">- Gestion de log avec promtail et loki (pas rÃ©ussi Ã  faire fonctionner loki)
- Segmentation rÃ©seau des vm pour avoir un rÃ©seau dÃ©diÃ© pour l'administration.
- Configuration des namespaces pour Ã©viter de mettre dans le dÃ©fault.
- Affiner les diffÃ©rents Ã©lÃ©ments configurÃ©s tout au long du projet.


ğŸš¨ğŸš¨ğŸš¨ Ajout d'une page HTML affichant directement le rapport (fait Ã  la toute fin)
</code></pre>
<img src="Pasted image 20250620000020.png" />
<img src="Pasted image 20250620000004.png" />
<hr>
<h2>Annexe 1 : ARBORESCENCE</h2>
<hr class="gradient-warm">
<pre><code class="language-bash">/
â””â”€â”€ Projet
    â”œâ”€â”€ Ansible
    â”‚Â Â  â””â”€â”€ hosts.ini
    â”œâ”€â”€ Project-docs
    â”‚Â Â  â”œâ”€â”€ Config
    â”‚Â Â  â”‚Â Â  â””â”€â”€ nginx
    â”‚Â Â  â”‚Â Â      â”œâ”€â”€ certs
    â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ server.crt
    â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ server.key
    â”‚Â Â  â”‚Â Â      â””â”€â”€ conf
    â”‚Â Â  â”‚Â Â          â””â”€â”€ default.conf
    â”‚Â Â  â”œâ”€â”€ docker-compose.yml
    â”‚Â Â  â”œâ”€â”€ Dockerfile
    â”‚Â Â  â”œâ”€â”€ Documentation
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Excalidraw
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ Drawing 2025-06-11 14.26.16.excalidraw.md
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250604214223.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250604214320.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250604214528.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted Image 20250611143455_879.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted Image 20250611144601_143.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250618151743.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250618152106.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250618155137.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250618155144.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250619110512.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250619110538.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250619110645.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pasted image 20250619112740.png
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Rapport.md
    â”‚Â Â  â”‚Â Â  â””â”€â”€ Rapport.pdf
    â”‚Â Â  â”œâ”€â”€ Kubernetes
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ App
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ grafana
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ grafana-deployment.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ grafana-service.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kube-metrics
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cluster-role-binding.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cluster-role.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ deployment.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kustomization.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ metric-service.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ service-account.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prometheus
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prometheus-deployment.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ prometheus-service.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ site
    â”‚Â Â  â”‚Â Â  â”‚Â Â      â”œâ”€â”€ site-deployment.yaml
    â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ site-service.yaml
    â”‚Â Â  â”‚Â Â  â””â”€â”€ Config-map
    â”‚Â Â  â”‚Â Â      â””â”€â”€ prometheus.yml
    â”‚Â Â  â”œâ”€â”€ Playbooks
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ All.yml
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ install-docker.yml
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ install-k8s-manifests.yml
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ install-k8s.yml
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mco.yaml
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ old
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ All.yml
    â”‚Â Â  â”‚Â Â  â””â”€â”€ setup-hosts.yml
    â”‚Â Â  â””â”€â”€ Website_content
    â”‚Â Â      â”œâ”€â”€ index.html
    â”‚Â Â      â”œâ”€â”€ page
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Dark PDF export.css
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ page.html
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250604214223.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250604214320.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250604214528.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted Image 20250611143455_879.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted Image 20250611144601_143.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250618151743.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250618152106.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250618155137.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250618155144.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250619110512.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250619110538.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250619110645.png
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ Pasted image 20250619112740.png
    â”‚Â Â      â”‚Â Â  â””â”€â”€ supercharged-links-gen.css
    â”‚Â Â      â”œâ”€â”€ rapport.pdf
    â”‚Â Â      â”œâ”€â”€ script.js
    â”‚Â Â      â””â”€â”€ style.css
    â””â”€â”€ SMB111- Projet - 2024 - 2025.docx.pdf
</code></pre>
</body>
</html>