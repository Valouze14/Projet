# DÃ©ploiement AutomatisÃ© d'un Site de Documentation avec MCO/MCS

<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

## Phase 1 : PrÃ©paration des Machines                      
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

### ğŸ”¹ Ã‰tape 1 : Installation de l'OS

âœ… **Choix de l'OS (Ubuntu/Debian recommandÃ©)** : 
```conf
Debian 12` (recommandÃ©) 
``` 
ğŸ› ï¸ğŸ” **Installation de base avec SSH activÃ©** : 
```conf
Installation par dÃ©faut avec la supression de la partition swap pour kubernetes 
```
 **SSH activÃ©** :

```bash
ssh.service - OpenBSD Secure Shell server
Loaded: loaded (/lib/systemd/system/ssh.service; enabled)
Active: active (running) since Tue 2025-04-15 09:29:13 CEST; 5min ago
```

ğŸ“¦ **Mise Ã  jour des paquets** :
```bash
apt-get update
apt-get upgrade
```

---
<div style="page-break-after: always;"></div>

### ğŸ”¹ Ã‰tape 2 : Configuration initiale des machines

ğŸ‘¥ **Configuration du PATH (commandes manquantes)** :
```bash
export PATH=$PATH:/usr/sbin #Ajout de ce dossier au path pour avoir des commandes nÃ©cessaire Ã  la suite du projet
```

ğŸ‘¥ **Ajout des utilisateurs** :
```bash
adduser val_master       # VM Master
adduser val_worker1      # VM Worker Node 1
adduser val_worker2      # VM Worker Node 2
```

ğŸ‘¥ **Ajout des utilisateurs au groupe sudo** :
```bash
usermod -aG sudo val_master      # VM Master
usermod -aG sudo val_worker1     # VM Worker Node 1
usermod -aG sudo val_worker2      # VM Worker Node 2
```

ğŸŒ **Configuration rÃ©seau & hostnames** :
```bash
hostnamectl set-hostname Master     #VM Master
hostnamectl set-hostname Worker1    #VM Worker Node 1
hostnamectl set-hostname Worker2    #VM Worker Node 2
```

ğŸ” **GÃ©nÃ©ration de clÃ© SSH + copie** :
```bash
ssh-keygen -t rsa -b 4096   # Avec passphrase "valentin"

ssh-copy-id val_worker1@192.168.142.144
ssh-copy-id val_worker2@192.168.142.143
ssh-copy-id val_master@192.168.142.137
```

ğŸ§¾ **RÃ©solution de nom** (`/etc/hosts`) :
```bash
192.168.142.137 Master
192.168.142.144 Worker1
192.168.142.143 Worker2
```

<div style="page-break-after: always;"></div>

ğŸ§¾ **Fichier de configuration SSH ** (`/etc/ssh/sshd_config`) :
```bash
PubKeyAuthentication yes
PermitRootLogin no
PasswordAuthentication no
```

ğŸ§ª **VÃ©rification SSH sans mot de passe via des clÃ©s publiques** :
```bash
ssh val_worker1@192.168.142.142
# âœ Enter passphrase...

ssh root@192.168.142.142
# âœ Permission denied (clÃ© requise)
```

---

### ğŸ”¹ Ã‰tape 3 : Installation des prÃ©requis

ğŸ **Installation de python ** :
```bash
sudo apt install python3 python3-pip
```

âœ… **VÃ©rification de la connectivitÃ© entre les machines** :
```bash
val_master# ping 192.168.142.144
val_master# ping 192.168.142.143
```
---
<div style="page-break-after: always;"></div>
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

## Phase 2 : Installation & Configuration de Docker

<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

### ğŸ”¹ Ã‰tape 1 : Installation de Docker

ğŸ“š [Guide IT-Connect Docker Debian](https://www.it-connect.fr/installation-pas-a-pas-de-docker-sur-debian-11/)
```bash
sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
sudo curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
sudo echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io
sudo systemctl enable docker
sudo systemctl status docker
```

---

### ğŸ”¹ Ã‰tape 2 : Ajout des utilisateurs Ã  Docker (pour Ã©viter lâ€™utilisation de sudo)

ğŸ‘¤ **Ajouter l'utilisateur au groupe `docker`** :
```bash
usermod -aG docker ${USER}
```

---

### ğŸ”¹ Ã‰tape 3 : VÃ©rification du bon fonctionnement de Docker

âœ… **Tester le fonctionnement** :
```bash
docker run hello-world
```

---
<div style="page-break-after: always;"></div>
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

## Phase 3 : DÃ©ploiement de Kubernetes

<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

### ğŸ”¹ Ã‰tape 1 : Installation de Kubernetes (kubeadm, kubelet, kubectl)

ğŸ› ï¸ **PrÃ©paration** :
```bash
sudo modprobe br_netfilter
```

ğŸ“š [Installer Kubernetes via kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)  
```bash
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
sudo systemctl enable --now kubelet
```

ğŸ“¦ [TÃ©lÃ©charger cri-dockerd](https://github.com/Mirantis/cri-dockerd/releases/tag/v0.3.17)
```bash
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.17/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
sudo dpkg -i cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
```

---

### ğŸ”¹ Ã‰tape 2 : Initialisation du cluster

ğŸš€ **Initialiser le cluster** :
```bash
sudo kubeadm init --cri-socket unix:///var/run/cri-dockerd.sock
```

ğŸ”§ **Configurer kubectl** :

```bash
# Si on est en root
export KUBECONFIG=/etc/kubernetes/admin.conf

# Si on est pas root
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

```

---

### ğŸ”¹ Ã‰tape 3 : Ajout des nÅ“uds au cluster

**Connexion des workers au noeud master** :
```bash
kubeadm join 192.168.142.137:6443 --token lyu32e.828538t417gn9omw \
	--discovery-token-ca-cert-hash sha256:d49e4ea40e6f390e95ad78ce10441809a5a8fb56eccbbac2f33f89fb2ea81137 --cri-socket unix:///var/run/cri-dockerd.sock
```

ğŸŒ **DÃ©ployer Flannel (rÃ©seau Pods)** :
```bash
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml # Sur le master
```

---

### ğŸ”¹ Ã‰tape 4 : VÃ©rification du bon fonctionnement du cluster

ğŸ“‹ **VÃ©rification** :
```bash
kubectl get nodes
kubectl get pods
```

---

<div style="page-break-after: always;"></div>
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

## Phase 4 : DÃ©ploiement du Site Web
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

### ğŸ”¹ Ã‰tapes 1 : Ã‰criture dâ€™un Dockerfile (pour contenir lâ€™application web statique)

ğŸ“ **Ã‰criture du `Dockerfile` pour app web statique** :
```bash
# Utiliser la derniÃ¨re image officielle de Nginx
FROM nginx:latest

# Copier le rÃ©pertoire contenant le html,css,js dans le rÃ©pertoire de service de Nginx
COPY Website_content /usr/share/nginx/html 

# Copier notre default.conf dans le rÃ©pertoire de service de Nginx
COPY Config/nginx/conf/default.conf /etc/nginx/conf.d/default.conf

# Copier le fichier PDF dans le rÃ©pertoire de service de Nginx
COPY Website_content/rapport.pdf /usr/share/nginx/html/mon_fichier.pdf

# Copier les certificats SSL dans le rÃ©pertoire de service de Nginx
COPY Config/nginx/certs/server.crt /etc/nginx/certs/server.crt
COPY Config/nginx/certs/server.key /etc/nginx/certs/server.key
```

---

### ğŸ”¹ Ã‰tapes 2 : CrÃ©ation dâ€™un docker-compose.yml pour lâ€™orchestration locale

ğŸ“œ **CrÃ©ation du `docker-compose.yml`** :
```bash
services:
  static-app:
    build: .
    ports:
      - "80:80"
      - "443:443"
    restart: always
```

---

### ğŸ”¹ Ã‰tapes 3 : DÃ©ploiement initial avec Docker Compose pour validation

ğŸ§ª **DÃ©ploiement local avec Docker Compose** :
```bash
docker compose up --build
```

âœ…** Test des accÃ¨s au site :

https://mon-site-local.test:443
http://mon-site-local.test:80

![[Pasted image 20250604214223.png]]

![[Pasted image 20250604214528.png]]

---

### ğŸ”¹ Ã‰tapes 4 : CrÃ©ation des manifests Kubernetes pour le dÃ©ploiement sur le cluster

ğŸ³ **Push de l'image docker issue du `Dockerfile` de l'Ã©tape 1 sur docker hub afin de pouvoir l'utiliser directement dans les pods.

ğŸ“œ **CrÃ©ation du  deployment`site-deployment.yaml`** :

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: site-doc-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: site-doc
  template:
    metadata:
      labels:
        app: site-doc
    spec:
      containers:
      - name: site
        image: valouze14/valouze14:latest
        ports:
        - containerPort: 80
        - containerPort: 443
```

ğŸ“œ **CrÃ©ation du service `site-service.yaml`** :

```bash
apiVersion: v1
kind: Service
metadata:
  name: site-doc-service 
spec:
  selector:
    app: site-doc # Cible tous les pods qui ont le labels site-doc
  type: NodePort
  ports:
    - name: http
      protocol: TCP
      port: 8080 # Le port sur lequel le service Ã©coutera Ã  l'intÃ©rieur du cluster
      targetPort: 80 # Le port sur lequel votre application Ã©coute Ã  l'intÃ©rieur du pod (port 80 de votre conteneur)
      nodePort: 30080 # Un port entre 30000 et 32767. Choisissez-en un disponible.
    - name: https
      protocol: TCP
      port: 4443
      targetPort: 443
      nodePort: 30443
```
---

### ğŸ”¹ Ã‰tapes 5 : DÃ©ploiement final sur Kubernetes 

ğŸš€ **DÃ©ploiement final sur Kubernetes** :

```bash
kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/Site
```

âœ…**Test des accÃ¨s au site :**

https://mon-site-local.test2:30443
http://mon-site-local.test2:30080

![[Pasted image 20250618151743.png]]
![[Pasted image 20250618152106.png]]

âœ…VÃ©rification load balancer :**

Par dÃ©faut, le service distribue sans affinitÃ© aux pods associÃ©s ce qui fait un load-balancer naturel, je peux vÃ©rifier en regardant en direct les logs des trois poids qui hÃ©berge mon site. A chaque fois, il faut supprimer les donnÃ©es dans le cache pour Ã©tablir une nouvelle session. Je constate qu'Ã  chaque nouvelle session, les requÃªtes n'arrivent jamais sur le mÃªme pod.

```bash
kubectl logs -f site-doc-deployment-7887cccc5b-mp5vt
```

![[Pasted image 20250619112740.png]]

---
<div style="page-break-after: always;"></div>
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

## Phase 5 : Automatisation avec Ansible

<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

ğŸ“¦ **DÃ©pÃ´t des ressources sur Github**
```bash
git clone https://github.com/Valouze14/Projet.git
```

ğŸ“œ**CrÃ©ation du fichier d'inventories.ini** :
```bash
[all]
192.168.142.137 ansible_user=val_master # VM Master
192.168.142.144 ansible_user=val_worker1 # VM Worker Node 1
192.168.142.143 ansible_user=val_worker2 # VM Worker Node 2

[masters]
192.168.142.137

[workers]
192.168.142.144
192.168.142.143
```
### ğŸ”¹ Ã‰tapes 1 : CrÃ©ation des playbooks Ansible pour automatiser lâ€™installation et la configuration



ğŸ“œ **CrÃ©ation du fichier setup-hosts.yml** :

```bash

- name: Setup Hosts
  hosts: all
  become: true
  tasks:

############################################################
# Ajouter les entrÃ©es des hÃ´tes et le nom de domaine dans le fichier /etc/hosts
############################################################
  - name: DÃ©finir les entrÃ©es des hÃ´tes + nom de domaine du site web
    blockinfile:
      path: /etc/hosts
      block: |
        192.168.142.137 Master
        192.168.142.144 Worker1
        192.168.142.143 Worker2
        192.168.142.137 mon-site-local.test
      backup: true

############################################################
# DÃ©finir le nom dâ€™hÃ´te pour la VM Master
############################################################
  - name: DÃ©finir le nom d hÃ´te pour la VM Master
    hostname:
      name: Master
    when: inventory_hostname == '192.168.142.137'

############################################################
# DÃ©finir le nom dâ€™hÃ´te pour la VM Worker1
############################################################
  - name: DÃ©finir le nom d hÃ´te pour la VM Worker 1
    hostname:
      name: Worker1
    when: inventory_hostname == '192.168.142.144'

############################################################
# DÃ©finir le nom dâ€™hÃ´te pour la VM Worker2
############################################################
  - name: DÃ©finir le nom d hÃ´te pour la VM Worker 2
    hostname:
      name: Worker2
    when: inventory_hostname == '192.168.142.143'

############################################################
# Installer Python 3 et pip (prÃ©requis pour d autres tÃ¢ches Ansible)
############################################################
  - name: Installer python3 et python3-pip
    apt:
      name:
        - python3
        - python3-pip

############################################################
# Charger le module noyau br_netfilter (nÃ©cessaire pour Kubernetes)
############################################################
  - name: Charger le module br_netfilter
    modprobe:
      name: br_netfilter
      state: present

############################################################
# Mettre Ã  jour la liste des paquets et effectuer une mise Ã  jour complÃ¨te
############################################################
  - name: Mise Ã  jour de la liste des paquets
    apt:
      update_cache: true
      upgrade: dist
```


ğŸ“œ **CrÃ©ation du fichier install-docker.yml** :

```bash
- name: Install Docker 
  hosts: all
  become: true
  tasks:

############################################################
# Installer les paquets nÃ©cessaires au fonctionnement de Docker
############################################################
  - name: Installer les paquets nÃ©cessaires pour Docker
    apt:
      name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg2
      - software-properties-common
      state: present
      update_cache: true

############################################################
# Ajouter la clÃ© GPG officielle de Docker pour sÃ©curiser le dÃ©pÃ´t
############################################################
  - name: Ajouter la clÃ© GPG officielle de Docker
    shell: curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    args:
      creates: /usr/share/keyrings/docker-archive-keyring.gpg

############################################################
# Ajouter le dÃ©pÃ´t officiel Docker Ã  la liste des sources APT
############################################################
  - name: Ajouter le dÃ©pÃ´t Docker Ã  la liste des sources APT
    lineinfile:
      path: /etc/apt/sources.list.d/docker.list
      line: deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian {{ ansible_distribution_release }} stable
      create: true
      mode: '0644'

############################################################
# Mettre Ã  jour lâ€™index des paquets aprÃ¨s ajout du dÃ©pÃ´t Docker
############################################################
  - name: Mettre Ã  jour l'index des paquets aprÃ¨s l'ajout du dÃ©pÃ´t Docker
    apt:
      update_cache: true

############################################################
# Installer Docker Engine, l'interface en ligne de commande et containerd
############################################################
  - name: Installer Docker Engine, CLI et Containerd
    apt:
      name:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      state: present

############################################################
# Activer et dÃ©marrer le service Docker au dÃ©marrage de la machine
############################################################
  - name: VÃ©rifier que le service Docker est dÃ©marrÃ© et activÃ© au dÃ©marrage
    systemd:
      name: docker
      state: started
      enabled: true

############################################################
# VÃ©rifier le statut du service Docker (diagnostic uniquement)
############################################################
  - name: VÃ©rifier le statut du service Docker (pour information, ne modifie rien)
    command: systemctl status docker
    register: docker_status

############################################################
# Afficher le statut du service Docker dans la sortie Ansible
############################################################
  - name: Afficher le statut du service Docker
    debug:
      var: docker_status.stdout_lines

############################################################
# Ajouter le user val_master au groupe docker (accÃ¨s sans sudo)
############################################################
  - name: Ajouter val_master au groupe docker
    user:
      name: val_master
      groups: docker
      append: true
    when: inventory_hostname == '192.168.142.137'

############################################################
# Ajouter le user utilisateur val_worker1 au groupe docker
############################################################
  - name: Ajouter val_worker1 au groupe docker
    user:
      name: val_worker1
      groups: docker
      append: true
    when: inventory_hostname == '192.168.142.144'

############################################################
# Ajouter l'utilisateur val_worker2 au groupe docker
############################################################
  - name: Ajouter val_worker2 au groupe docker
    user:
      name: val_worker2
      groups: docker
      append: true
    when: inventory_hostname == '192.168.142.143'

############################################################
# VÃ©rifier que Docker fonctionne avec un conteneur de test
############################################################
  - name: ExÃ©cuter docker run hello-world pour vÃ©rifier la bonne installation
    command: docker run hello-world
    become: false
    register: docker_hello_world_result

############################################################
# Afficher le rÃ©sultat du test docker run hello-world
############################################################
  - name: Afficher le rÃ©sultat de docker run hello-world
    debug:
      var: docker_hello_world_result.stdout_lines

############################################################
# TÃ©lÃ©charger le binaire cri-dockerd depuis GitHub
############################################################
  - name: TÃ©lÃ©charger le paquet cri-dockerd
    get_url:
      url: https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.17/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
      dest: /tmp/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb
      mode: '0644'

############################################################
# Installer le paquet cri-dockerd tÃ©lÃ©chargÃ© localement
############################################################
  - name: Installer le paquet cri-dockerd
    ansible.builtin.apt:
      deb: /tmp/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb

############################################################
# Lancer Docker Compose pour dÃ©ployer les conteneurs du projet
############################################################
  - name: Lancer docker compose up avec le rÃ©pertoire de projet spÃ©cifiÃ©
    ansible.builtin.command: docker compose up --build -d
    args:
      chdir: /home/val_master/Documents/Projet/Project-docs/   # SpÃ©cifie le rÃ©pertoire de travail
    when: inventory_hostname == '192.168.142.137' # ExÃ©cuter uniquement sur le nÅ“ud maÃ®tre
```

ğŸ“œÂ **CrÃ©ation du fichier install-k8s.yml**Â :

```bash
############################################################
# 1. Installation des prÃ©requis Kubernetes
############################################################
- name: Installer les paquets et composants Kubernetes
  hosts: all
  become: true
  tasks:
    - name: Installer les paquets nÃ©cessaires
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gpg
        state: present

    - name: Ajouter la clÃ© GPG officielle de Kubernetes
      shell: >
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key |
        gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: Ajouter le dÃ©pÃ´t Kubernetes
      lineinfile:
        path: /etc/apt/sources.list.d/kubernetes.list
        line: deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
        create: true
        mode: '0644'

    - name: Mise Ã  jour des paquets
      apt:
        update_cache: true

    - name: Installer kubelet, kubeadm et kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present

    - name: Marquer kubelet, kubeadm et kubectl en hold
      dpkg_selections:
        name: '{{ item }}'
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: Activer et dÃ©marrer le service kubelet
      systemd:
        name: kubelet
        enabled: true
        state: started


############################################################
# 2. Initialisation du master Kubernetes
############################################################
- name: Initialiser le master Kubernetes
  hosts: 192.168.142.137
  become: true
  tasks:
    - name: Initialiser le cluster avec kubeadm
      command: kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///var/run/cri-dockerd.sock
      tags: [kubeadm_init]

    - name: CrÃ©er .kube/config pour le user val_master
      shell: |
        mkdir -p /home/val_master/.kube
        cp -i /etc/kubernetes/admin.conf /home/val_master/.kube/config
        chown val_master:val_master /home/val_master/.kube/config

    - name: Appliquer le manifeste Flannel CNI
      ansible.builtin.shell:
        kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
      become: false

############################################################
# 3. RÃ©cupÃ©ration de la commande de jointure
############################################################
- name: Obtenir la commande de jointure
  hosts: 192.168.142.137
  become: true
  tasks:
    - name: GÃ©nÃ©rer la commande kubeadm join
      command: kubeadm token create --print-join-command
      register: join_command
      run_once: true

    - name: Sauvegarder la commande dans un fact partagÃ©
      set_fact:
        kube_join_cmd: "{{ join_command.stdout }} --cri-socket unix:///var/run/cri-dockerd.sock"
      run_once: true


############################################################
# 4. Faire joindre les workers au cluster
############################################################
- name: Faire rejoindre les workers au cluster Kubernetes
  hosts: all
  become: true
  tasks:
    - name: Joindre les workers (sauf master)
      command: "{{ hostvars['192.168.142.137'].kube_join_cmd }}"
      when: inventory_hostname != '192.168.142.137'
      register: kubeadm_join_output_workers
      tags: [kubeadm_join_workers]


############################################################
# (Facultatif) Copier admin.conf depuis le master si nÃ©cessaire
############################################################
- name: Copier admin.conf sur les workers depuis localhost (optionnel)
  hosts: workers
  become: true
  tasks:
    - name: Rendre admin.conf lisible en local
      file:
        path: /etc/kubernetes/admin.conf
        mode: '0644'
        owner: root
        group: root
      delegate_to: localhost
      run_once: true

    - name: Copier admin.conf sur les workers
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /etc/kubernetes/admin.conf
        owner: root
        group: root
        mode: '0644'

############################################################
# 5. Configuration de .kube/config sur les workers (facultatif)
############################################################
- name: Configurer kubectl sur les workers (facultatif)
  hosts: workers
  become: true
  tasks:
    - name: CrÃ©er le rÃ©pertoire .kube et copier le fichier admin.conf (Master uniquement)
      ansible.builtin.shell: |
        mkdir -p /home/val_worker1/.kube
        sudo cp -i /etc/kubernetes/admin.conf /home/val_worker1/.kube/config
        sudo chown val_worker1:val_worker1 /home/val_worker1/.kube/config
      when: inventory_hostname == '192.168.142.144'

    - name: CrÃ©er le rÃ©pertoire .kube et copier le fichier admin.conf (Master uniquement)
      ansible.builtin.shell: |
        mkdir -p /home/val_worker2/.kube
        sudo cp -i /etc/kubernetes/admin.conf /home/val_worker2/.kube/config
        sudo chown val_worker2:val_worker2 /home/val_worker2/.kube/config
      when: inventory_hostname == '192.168.142.143'
```

ğŸ“œÂ **CrÃ©ation du fichier install-k8s-manifests.yml**Â (inclus la phase 6 pour la supervision):

```bash
---
- name: DÃ©ploiement des outils de monitoring et des applications Kubernetes (sans collections externes)
  hosts: masters

  tasks:

############################################################
# CrÃ©er une ConfigMap Prometheus Ã  partir dâ€™un fichier de configuration local
############################################################
    - name: CrÃ©er la ConfigMap Prometheus
      ansible.builtin.command:
        kubectl create configmap prometheus-config --from-file=/home/val_master/Documents/Projet/Project-docs/Kubernetes/Config-map/prometheus.yml 

############################################################
# DÃ©ployer les manifests Kubernetes de l'application web (site)
############################################################
    - name: Appliquer les manifestes Kubernetes pour site
      ansible.builtin.command: kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/site

############################################################
# DÃ©ployer les manifests Kubernetes de Prometheus (monitoring)
############################################################
    - name: Appliquer les manifestes Kubernetes pour prometheus
      ansible.builtin.command: kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/prometheus

############################################################
# DÃ©ployer les manifests Kubernetes de Grafana (dashboard)
############################################################
    - name: Appliquer les manifestes Kubernetes pour grafana
      ansible.builtin.command: kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/grafana

############################################################
# DÃ©ployer les manifests Kubernetes de kube-metrics (exporter de mÃ©triques)
############################################################
    - name: Appliquer les manifestes Kubernetes pour kube-metrics
      ansible.builtin.command: kubectl apply -f /home/val_master/Documents/Projet/Project-docs/Kubernetes/App/kube-metrics


- name: Configurer kubectl sur les workers (facultatif)
  hosts: workers
  become: true

  tasks:

############################################################
# DÃ©marrer le conteneur cAdvisor pour surveiller les performances du nÅ“ud
############################################################
    - name: DÃ©marrer le conteneur cAdvisor
      ansible.builtin.shell: |
        docker run -d \
          --name=cadvisor \
          --restart=unless-stopped \
          --volume=/:/rootfs:ro \
          --volume=/var/run:/var/run:ro \
          --volume=/sys:/sys:ro \
          --volume=/var/lib/docker/:/var/lib/docker:ro \
          -p 8080:8080 \
          gcr.io/cadvisor/cadvisor
        # Ajout d'une vÃ©rification pour l'idempotence (pas strictement nÃ©cessaire pour 'docker run -d' car il ne relancera pas si le conteneur existe dÃ©jÃ )
      args:
        creates: /var/run/docker/cadvisor.pid

```
---

### ğŸ”¹ Ã‰tapes 2 : ExÃ©cution des playbooks pour provisionner lâ€™ensemble des machines

â–¶ï¸ **ExÃ©cution des playbooks sur le parc** : 

```bash
ansible-playbook Projet/Project-docs/Playbooks/All.yml -i Projet/Ansible/hosts.ini -K
```

---

### ğŸ”¹ Ã‰tapes 3 : Validation du bon fonctionnement

âœ… **Validation de lâ€™installation automatisÃ©e** : 

![[Pasted image 20250618155144.png]]
---

<div style="page-break-after: always;"></div>
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

## Phase 6 : Maintenance et Supervision

<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

### ğŸ”¹ Ã‰tapes 1 : Mise en place de la supervision des conteneurs et du cluster Kubernetes

#####  DÃ©ploiement de mÃ©triques des conteneurs avec `prometheus`** : 

ğŸ“œÂ **CrÃ©ation du config-map prometheus.yml**

```bash
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'cadvisor-worker1'
    static_configs:
      - targets: ['192.168.142.144:8080']

  - job_name: 'cadvisor-worker2'
    static_configs:
      - targets: ['192.168.142.143:8080']
  - job_name: k8s-kube-state-metrics-cluster
    honor_timestamps: true
    metrics_path: /metrics
    scheme: http
    static_configs:
      - targets: ['192.168.142.137:30000']
    metric_relabel_configs:
    - target_label: cluster
      replacement: YourDefinedK8scluster
```

ğŸ“œÂ **CrÃ©ation du deployment prometheus-deployment.yaml**

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus
        args:
          - "--config.file=/etc/prometheus/prometheus.yml"
        ports:
          - containerPort: 9090
        volumeMounts:
          - name: config-volume
            mountPath: /etc/prometheus/
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-config
```

ğŸ“œÂ **CrÃ©ation du service prometheus-service.yaml**

```bash
apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
spec:
  selector:
    app: prometheus
  ports:
    - protocol: TCP
      port: 9090
      targetPort: 9090
      nodePort: 30200 
  type: NodePort
```
##### Mise en place de la supervision avec `grafana`** : 

ğŸ“œÂ **CrÃ©ation du deployment grafana-deployment.yaml**

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana
          ports:
            - containerPort: 3000
```

ğŸ“œÂ **CrÃ©ation du service grafana-service.yaml**

```bash
apiVersion: v1
kind: Service
metadata:
  name: grafana-service
spec:
  selector:
    app: grafana
  ports:
    - protocol: TCP
      port: 3000
      targetPort: 3000
      nodePort: 30500
  type: NodePort
```

---
### ğŸ”¹ Ã‰tapes 2 : DÃ©ploiement de mÃ©triques et logs pour surveiller lâ€™infrastructure

 ğŸ³ **DÃ©ploiement de mÃ©triques des conteneurs avec `cAdvisor`** : 

```bash 
docker run -d \
  --name=cadvisor \
  --restart=unless-stopped \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  -p 8080:8080 \
  gcr.io/cadvisor/cadvisor
```

ğŸ“ˆ **DÃ©ploiement de mÃ©triques du cluster Kubernetes avec le service `kube-state-metric`** : 
	
ğŸ“¦ https://github.com/kubernetes/kube-state-metrics/tree/main/examples/standard

âœ… **VÃ©rification de la configuration de `prometheus` :
![[Pasted image 20250619110512.png]]
âœ… **VÃ©rification de la supervision avec `graphana` :

ID template utilisÃ© pour la supervision du cluster Kubernetes : 13332

![[Pasted image 20250619110538.png]]

ID template utilisÃ© pour la supervision les conteneurs : 14282

![[Pasted image 20250619110645.png]]

---

### ğŸ”¹ Ã‰tapes 3 : Automatisation de la gestion des mises Ã  jour avec Ansible

ğŸ” **Automatisation des MAJ avec Ansible** : 





<div style="page-break-after: always;"></div>
<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

## Axes d'amÃ©liorations

<hr class="gradient-warm">    <!-- Ligne jaune/orange -->

```bash
- Segmentation rÃ©seau des vm pour avoir un rÃ©seau dÃ©diÃ© pour l'administration.
- Configuration des namesspaces pour Ã©viter de mettre dans le dÃ©fault.
- 
```
---
## Annexe 1 : ARBORESCENCE

 <hr class="gradient-warm">    <!-- Ligne jaune/orange -->

```bash
/
â”œâ”€â”€ etc/
â”‚   â””â”€â”€ Ansible/
â”‚       â””â”€â”€ hosts.ini                # Fichier d'inventaire Ansible
â”‚
â”œâ”€â”€ var/
â”‚   â””â”€â”€ log/                         # RÃ©pertoire des logs (ex. collecte centralisÃ©e)
â”‚
â”œâ”€â”€ Project-docs/                    # RÃ©pertoire principal du projet DocHub
â”‚     â”œâ”€â”€ Playbooks/                 # Playbooks Ansible (installation,config,gestion)
â”‚     â”‚   â”œâ”€â”€ setup_machines.yml     # PrÃ©paration initiale (OS, utilisateurs, SSH)
â”‚     â”‚   â”œâ”€â”€ install_docker.yml     # Installation et configuration de Docker 
â”‚     â”‚   â”œâ”€â”€ deploy_kubernetes.yml  # DÃ©ploiement de Kubernetes 
â”‚     â”‚   â”œâ”€â”€ deploy_website.yml     # DÃ©ploiement complet du site web 
â”‚     â”‚   â”œâ”€â”€ update_management.yml  # MCO : automatisation des mises Ã  jour
â”‚     â”‚   â””â”€â”€ ...                    # Autres playbooks (supervision,durcissement,etc.)
â”‚     â”‚
â”‚     â”œâ”€â”€ kubernetes_manifests/      # Manifests Kubernetes pour le site web
â”‚     â”‚   â”œâ”€â”€ deployment.yaml
â”‚     â”‚   â”œâ”€â”€ service.yaml
â”‚     â”‚   â”œâ”€â”€ ingress.yaml        
â”‚     â”‚   â””â”€â”€ ...
â”‚     â”‚
â”‚     â”œâ”€â”€ Docker/                    # Dockerfile & docker-compose
â”‚     â”‚   â”œâ”€â”€ Dockerfile             # Pour l'app web statique
â”‚     â”‚   â””â”€â”€ docker-compose.yml     # Orchestration locale
â”‚     â”‚
â”‚     â”œâ”€â”€ Website_content/           # Fichiers du site web statique
â”‚     â”‚   â”œâ”€â”€ index.html
â”‚     â”‚   â”œâ”€â”€ style.css
â”‚     â”‚   â”œâ”€â”€ script.js
â”‚     â”‚   â””â”€â”€ rapport.pdf            # Dossier pour le rapport en PDF
â”‚     â”‚
â”‚     â”œâ”€â”€ Documentation/             # Docs Markdown pour le projet
â”‚     â”‚   â”œâ”€â”€ rapport.md  
â”‚     â”‚
â”‚     â”œâ”€â”€ Scripts/                   # Scripts utilitaires (bash, python, etc.)
â”‚     â”‚
â”‚     â””â”€â”€ Config/                    # Fichiers de configuration divers
â”‚         â”œâ”€â”€ nginx/                 # Config spÃ©cifique Ã  Nginx
â”‚         â”‚   â”œâ”€â”€ certs
â”‚         â”‚   â”‚   â”œâ”€â”€ server.crt
â”‚         â”‚   â”‚   â””â”€â”€ server.key
â”‚         â”‚   â””â”€â”€ conf
â”‚         â”‚       â””â”€â”€ default.conf
â”‚         â””â”€â”€ ansible                    # Autres configs (monitoring, logs, etc.)
â”‚             â””â”€â”€ ansible.conf
â”œâ”€â”€ home/
â”‚   â””â”€â”€ val_worker/                  # RÃ©pertoire personnel de lâ€™utilisateur
â”‚       â””â”€â”€ .ssh/
â”‚           â”œâ”€â”€ id_rsa               # ClÃ© privÃ©e SSH (accÃ¨s sans mot de passe)
â”‚           â””â”€â”€ id_rsa.pub           # ClÃ© publique SSH


```

<div style="page-break-after: always;"></div>